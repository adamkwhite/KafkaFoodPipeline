# Kafka Food Processing Pipeline - Environment Configuration Template
# Copy this file to .env and fill in your actual values
# NEVER commit .env to version control (contains secrets)

# ==============================================================================
# KAFKA CONFIGURATION
# ==============================================================================

# KAFKA_BOOTSTRAP_SERVERS: Comma-separated list of Kafka broker addresses
# - "Bootstrap" servers are the initial contact points for Kafka clients
# - Clients discover other brokers from these initial connections
# - Format: host:port (9092 is standard Kafka port)
# - Local: Uses Docker service name 'kafka' (Docker networking resolves this)
# - AWS: Use actual EC2 instance IP or hostname
KAFKA_BOOTSTRAP_SERVERS=kafka:9092

# KAFKA_TOPIC_ORDERS: The topic where food orders are published/consumed
# - Topics are Kafka's way of organizing message streams (like database tables)
# - Producers write to topics, consumers read from topics
# - This topic will have 3 partitions for parallel processing
KAFKA_TOPIC_ORDERS=food-orders

# KAFKA_CONSUMER_GROUP: Consumer group ID for order processors
# - Consumer groups enable parallel processing and load balancing
# - Multiple consumers in same group share partition workload
# - Each partition assigned to only ONE consumer in a group
# - Example: 3 partitions, 2 consumers â†’ one gets 2 partitions, one gets 1
KAFKA_CONSUMER_GROUP=order-processors

# KAFKA_AUTO_OFFSET_RESET: What to do when consumer has no committed offset
# - 'earliest': Start reading from beginning of topic (all historical messages)
# - 'latest': Only read new messages published after consumer starts
# - For learning/development, use 'earliest' to see all messages
# - Production typically uses 'latest' to avoid reprocessing old data
KAFKA_AUTO_OFFSET_RESET=earliest

# KAFKA_ENABLE_AUTO_COMMIT: Whether consumer automatically commits offsets
# - 'false' (recommended): Manual commit after successful processing
# - 'true': Auto-commit at intervals (risk of message loss on crash)
# - Manual commit ensures "at least once" processing semantics
KAFKA_ENABLE_AUTO_COMMIT=false

# ==============================================================================
# DATABASE CONFIGURATION
# ==============================================================================

# PostgreSQL connection settings for order storage

# DB_HOST: Database server hostname
# - Local: Docker service name 'postgres'
# - AWS: Could use RDS endpoint or localhost if PostgreSQL in same EC2 instance
DB_HOST=postgres

# DB_PORT: PostgreSQL default port
DB_PORT=5432

# DB_NAME: Database name for order data
DB_NAME=food_orders

# DB_USER: Database username
# - Change this in production!
DB_USER=kafka_user

# DB_PASSWORD: Database password
# - CHANGE THIS IN PRODUCTION!
# - Use strong passwords (20+ characters, random)
# - Consider AWS Secrets Manager for production
DB_PASSWORD=kafka_password_local

# DB_POOL_SIZE: Number of database connections to maintain in pool
# - Connection pooling reuses connections instead of creating new ones
# - Value depends on expected load (10-20 is common)
# - For learning project with low volume, 5 is sufficient
DB_POOL_SIZE=5

# ==============================================================================
# PRODUCER CONFIGURATION
# ==============================================================================

# PRODUCER_CLIENT_ID: Unique identifier for producer instance
# - Visible in Kafka broker logs and monitoring tools
# - Helps track which producer sent which messages
PRODUCER_CLIENT_ID=order-producer

# PRODUCER_RATE: Number of orders to generate per second
# - Controls simulation speed and load testing
# - Start low (5-10) for debugging, increase for throughput testing
# - 10 ops/sec = 600 orders/minute = good for demo
PRODUCER_RATE=10

# PRODUCER_DURATION: How long to run producer (seconds)
# - 0 = run indefinitely (Ctrl+C to stop)
# - >0 = run for specified duration then stop
# - Useful for batch testing (e.g., generate 1000 orders then stop)
PRODUCER_DURATION=60

# PRODUCER_COMPRESSION: Message compression algorithm
# - none: No compression (fastest, largest size)
# - gzip: High compression, slower
# - snappy: Balanced (recommended for most use cases)
# - lz4: Fast compression
# - zstd: Modern, efficient compression
PRODUCER_COMPRESSION=snappy

# PRODUCER_LINGER_MS: Time to wait before sending batch (milliseconds)
# - 0 = send immediately (lowest latency)
# - >0 = wait to batch messages (higher throughput)
# - 10ms = good balance for this demo
PRODUCER_LINGER_MS=10

# PRODUCER_BATCH_SIZE: Maximum batch size in bytes
# - Larger batches = better compression and throughput
# - 16KB (16384) is Kafka default
PRODUCER_BATCH_SIZE=16384

# PRODUCER_BUFFER_MEMORY: Producer buffer memory in bytes
# - Total memory for buffering messages before sending
# - 32MB (33554432) is Kafka default
# - Increase if getting BufferError (producer too fast)
PRODUCER_BUFFER_MEMORY=33554432

# PRODUCER_ACKS: Acknowledgment level
# - 0 = No acknowledgment (fire and forget, fastest, no guarantee)
# - 1 = Leader acknowledgment (good balance)
# - all/-1 = All replicas (slowest, strongest guarantee)
PRODUCER_ACKS=1

# ENABLE_IDEMPOTENCE: Prevent duplicate messages
# - true = exactly-once semantics within partition (recommended)
# - false = at-least-once semantics (may have duplicates)
ENABLE_IDEMPOTENCE=true

# MOCK_SEED: Random seed for reproducible mock data
# - Same seed = same customers and orders every time
# - Useful for testing and debugging
# - 0 or empty = random seed (different data each run)
MOCK_SEED=42

# ==============================================================================
# LOGGING CONFIGURATION
# ==============================================================================

# LOG_LEVEL: Logging verbosity
# - DEBUG: Very detailed, use for development/troubleshooting
# - INFO: General informational messages (recommended for production)
# - WARNING: Only warnings and errors
# - ERROR: Only errors
LOG_LEVEL=INFO

# LOG_FORMAT: Log output format
# - 'json': Structured logs, easy to parse and search (recommended for production)
# - 'text': Human-readable format for local development
LOG_FORMAT=json

# ==============================================================================
# AWS DEPLOYMENT OVERRIDES
# ==============================================================================
# When deploying to AWS t2.small instance, override these values:
#
# KAFKA_BOOTSTRAP_SERVERS=localhost:9092  # Kafka running on same EC2 instance
# DB_HOST=localhost                        # PostgreSQL on same EC2 instance
# LOG_LEVEL=INFO                          # Less verbose for production
# ORDER_GENERATION_RATE=5                 # Lower rate for resource-constrained t2.small
#
# Memory constraints on t2.small (2GB RAM):
# - Kafka JVM heap: 512MB
# - PostgreSQL shared buffers: 256MB
# - Producer/Consumer: 128MB each
# - OS + other services: ~1GB remaining

# ==============================================================================
# DEVELOPMENT / TESTING OVERRIDES
# ==============================================================================
# For integration tests using testcontainers, these will be overridden
# programmatically with container-assigned ports
